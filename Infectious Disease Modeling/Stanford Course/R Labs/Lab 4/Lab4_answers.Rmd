---
title: 'Lab 4: Parameter Estimation and Uncertainty Analysis'
output:
  html_document: default
  pdf_document: default
---

First, let's read in the packages we will need. 

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(deSolve)
#uncomment the next line if you need to install mvtnorm.
#install.packages("mvtnorm") 
library(mvtnorm)

```

# Question 1. 
- Download a dataset on a measles outbreak in 3 communities in Niaemy, Niger (code below)
- Assume the population size (N) is 50,000 and that 80\% of the population starts off vaccinated (note: added via Canvas announcement)
- Refit the SIR model to community B
- Estimate $\beta$ and $\gamma$ for this community
- Describe your findings
- You should only need to change numbers in the above model (e.g. the duration of observation differs)

- Download a dataset on a measles outbreak in 3 communities in Niaemy, Niger
```{r}
niamey <- read.csv("http://kingaa.github.io/short-course/parest/niamey.csv")
niamey.b = niamey %>% filter(community=="B")

#let's take a look at the data
head(niamey.b)

#it looks like the time step is biweekly, indicated by the "biweek" variable, and measles cases are indicated by the "measles" variable

#plot the data
ggplot() +
  xlab('Time (biweek)') +
  ylab('Count') + 
  geom_point(data=niamey.b,aes(x=biweek,y=measles)) + theme_bw()

```

- Assume the population size (N) is 50,000 (and that 80\% of the population starts out vaccinated)
```{r}
ClosedXYZ<-function(t, state, parameters) {
  with(as.list(c(state, parameters)),{
    N = X + Y + Z
    dX <- -beta*X*Y/N 
    dY <- beta*X*Y/N - gamma*Y
    dZ <- gamma*Y
    list(c(dX, dY, dZ))
  }) 
}

state <- c(X = 9999, Y = 1, Z = 40000)

times <- seq(0, 16, by = 1)
```
We can use the same ClosedXYZ model from the in-class portion of this lab. However, we adjust the *state* vector for a population size of 50,000 (with 80\% vaccination coverage), and the *times* vector to account for the 16 data points in the dataset.

- Refit the SIR model to community B
- Estimate $\beta$ and $\gamma$ for this community
```{r}
#As in class, select a sequence of 20 values for beta and gamma across a plausible range
n= 20
betalist= seq(5.0,25.0,length=n)
gammalist= seq(0.2,2.0,length=n)

#Here we evaluate the first combination to create our dataframe
parameters <- c(beta= betalist[1],gamma= gammalist[1])
output <- ode(y = state, times = times, func = ClosedXYZ, parms = parameters)
results<- data.frame(output,beta= rep(betalist[1],length(times)),
                     gamma=rep(gammalist[1],length(times)))

#now we loop off the 400 combinations of beta and gamma
for (i in 1:n){
  for (j in 1:n){
    parameters <- c(beta= betalist[i],gamma= gammalist[j])
    output <- ode(y = state, times = times, func = ClosedXYZ, parms = parameters)
    output<- data.frame(output,beta= rep(betalist[i],length(times)),
                         gamma=rep(gammalist[j],length(times)))
    results= rbind(results,output)  
  }
}
```
How do we define a plausible range for $\beta$ and $\gamma$? We could start with a very wide range and gradually narrow the range based on the results from initial calibration, like we did in class. We could also base $\beta$ and $\gamma$ off of other epidemiological studies and models of measles. Keeling and Rohani report an estimate $\ R_0$ of measles of 16-18 (Table 2.1 on page 21) and average duration of infection of around 2 weeks (page 102, although this is in the context of an SEIR model). An $\ R_0$ of 17 and a $\gamma$ of 1/14 imply a $\beta$ of 3.4. **BUT** recall that our model time step is biweekly, so $\gamma$ and $\beta$ should also be biweekly rates instead of daily rates. Assuming biweekly means we observe cumulative cases for the past 2 weeks (not past half week), this means translates to a $\gamma$ of 1 and a $\beta$ of 17. We explore a wide range around these estimates to start.

First, let's look at the range of $\beta$ with an intermediate level of $\gamma$.
```{r}
#examine beta while keeping gamma constant
ggplot(subset(results,gamma==gammalist[10]),aes(x=time,y=Y,color=as.factor(beta))) + 
  geom_line() + theme_bw() + scale_color_discrete(name="beta")
```

Now let's examine a range of $\gamma$ while holding $\beta$ constant.
```{r}
ggplot(subset(results,beta==beta[10]),aes(x=time,y=Y,color=as.factor(gamma))) + 
  geom_line() + theme_bw() + scale_color_discrete(name="gamma")
```


Define our likelihood function to take combinations of $\beta$ and $\gamma$, as in class
```{r}
poisson.loglik <- function (b,g) {
  res <- results %>% filter(times %in% niamey.b$biweek & beta==b & gamma==g)
  Y= res$Y
  sum(dpois(x=niamey.b$measles,lambda=Y,log=TRUE))
}
```
We specify that the log likelihood should be calculated for the time steps in the model output that match the time steps in the data. 

Now we are ready to visualize the likelihood surface for these combinations of $\beta$ and $\gamma$. We'll do that by creating a two dimensional contour map, where the x-axis will be $\beta$, the y-axis will be $\gamma$, and the colors will signify the log likelihood values (higher values are a better fit).

```{r}
grid <- expand.grid(b=betalist,g=gammalist)
grid <- plyr::ddply(grid,~b+g,mutate,loglik=poisson.loglik(b,g))
grid <- subset(grid,is.finite(loglik))

ggplot(grid,aes(x=b,y=g,z=loglik)) +
  geom_contour_filled(breaks= -exp(seq(9,0,-0.2))) + scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand=c(0,0)) + xlab("beta") + ylab("gamma") + xlim(c(5, 25)) +
  labs(fill = "Log Likelihood")
```
The highest likelihood occurs when $\beta$ is in the range of 9-14 and $\gamma$ is in the range of 1 to 1.8. Now let's narrow our $\gamma$ and $\beta$ range based on these initial results and rerun calibration.
```{r}
n= 20
betalist= seq(9,14,length=n)
gammalist= seq(1,1.8,length=n)

parameters <- c(beta= betalist[1],gamma= gammalist[1])
output <- ode(y = state, times = times, func = ClosedXYZ, parms = parameters)
results<- data.frame(output,beta= rep(betalist[1],length(times)),
                     gamma=rep(gammalist[1],length(times)))
#now we loop off the 400 combinations of beta and gamma
for (i in 1:n){
  for (j in 1:n){
    parameters <- c(beta= betalist[i],gamma= gammalist[j])
    output <- ode(y = state, times = times, func = ClosedXYZ, parms = parameters)
    output<- data.frame(output,beta= rep(betalist[i],length(times)),
                         gamma=rep(gammalist[j],length(times)))
    results= rbind(results,output)  
  }
}

grid <- expand.grid(b=betalist,g=gammalist)
grid <- plyr::ddply(grid,~b+g,mutate,loglik=poisson.loglik(b,g))
grid <- subset(grid,is.finite(loglik))

ggplot(grid,aes(x=b,y=g,z=loglik)) +
  geom_contour_filled(breaks= -exp(seq(9,0,-0.2))) + scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand=c(0,0)) + xlab("beta") + ylab("gamma") +
  labs(fill = "Log Likelihood")
```

It looks like $\beta=10.75$, $\gamma=1.35$ is a pretty good fit. We could use these values or find the exact values by looking for the maximum log likelihood in the *grid* dataframe. Let's run the model using these parameters and plot them against the Niamey data. 

If we wanted, we could also rerun calibration again and see if we can obtain an even better fit. However, there is a limit on how perfect of a fit we can obtain using a simple SIR model. 
```{r}
beta_best <- grid$b[which.max(grid$loglik)]
gamma_best <- grid$g[which.max(grid$loglik)]
print(beta_best)
print(gamma_best)

parameters <- c(beta=beta_best, gamma=gamma_best)

output <- ode(y = state, times = times, func = ClosedXYZ, parms = parameters)
results <- data.frame(output)

ggplot() + geom_point(data=niamey.b,aes(x=biweek,y=measles, color="Data")) +
  geom_point(data=results, aes(x=time, y=Y, color="Model Output")) +
  geom_line(data=results, aes(x=time, y=Y, color="Model Output")) +
  labs(x="Time (biweek)", y="Case Count", color="") + theme_bw()

```


- Describe your findings

The best-fitting $\beta$ and $\gamma$ from this calibration with a closed SIR model were 10.75 and 1.35 respectively (note - you may have run calibration correctly and gotten different results, depending on your starting ranges, iterations, etc.). These parameter values imply an$\ R_0$ of 8 and an average duration of infectiousness of 10.4 days. Compared to what we know about measles, this seems like a low $\ R_0$. There are several possible explanations for this, including (but not limited to): 1) We force the data to fit a closed SIR model, but the real infection dynamics of measles are likely more complex (for example there is an exposed period when individuals are not yet infectious or symptomatic and thus an SEIR model may be more appropriate). Other added complications could include case detection, lags in detection, etc. (or that we have misinterpreted the definition of "biweek"!) 2) We chose 80\% baseline vaccination coverage, but in practice this value may be higher. If we try rerunning the above code with 90\% of the population starting off immune, the resulting best-fitting $\beta$ is higher and the $\gamma$ is lower, implying longer duration of infection and higher $\ R_0$, which aligns better with our prior beliefs about these parameters. This is one reason why we would want to base the proportion of the population starting off immune on data (or calibrate it). 

Additionally, the best-fitting $\beta$ and $\gamma$ from calibration underestimate the number of cases at the infection's peak. This could be explained by some of the above factors (a more realistic model or different initial compartment sizes may result in calibrated parameters with better fit) as well as randomness and noise. Models will rarely achieve a perfect fit to calibration targets. 



2. Here are the case count data from Santa Clara county after March 1. If we visualize them, we get a nice epidemic curve, but we also believe that this was not due reaching a critical threshold of immunity as in the flu example above, but rather that parameters were changing. Describe (doing it is completely optional) how you could modify the XYZ model code above to estimate how changing parameter values during the past two months could explain the dynamics seen in the figure below. What parameter(s) could have changed and how? How could we include that in the model and estimate those parameter values? Keep it simple.

```{r}
fulldata<-read.csv('https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv')

data <- fulldata %>%
  filter(county=="Santa Clara") %>%
  mutate(date= as.Date(date,"%Y-%m-%d"))

inc <- diff(data$cases)
inc.2 <- inc[30:length(inc)] #March 1
plot(inc.2,type='b', xlab='days',ylab='new cases')

```

There are a few observations we can make from this graph that are inconsistent with our expectations of an SIR model without demography. The first is that initial growth is fast (possibly exponential through around day 30, although there's a lot of noise) and then it levels out. The second is that there is an uptick in new cases starting around day 70, which we would not expect to see in an SIR model without demography. Additionally, the epidemic peak (at around 200 new cases around day 30) is far smaller than the population of Santa Clara county (almost 2 million), but in a population of susceptibles we would expect a higher proportion of the population to get infected (unless perhaps the $\ R_0$ is barely above 1, which does not appear to be the case for Covid-19).

The likely explanation for these dynamics is that contact patterns, and thus the effective contact rate, and transmission, have changed as a result of awarness of the epidemic, social distancing, and the shelter-in-place order. One relatively simple change we could make to our SIR model to incorporate these changing contact patterns is to allow $\beta$ to vary over time, either directly or through a time-varying multiplier that represents the effect of social distancing (e.g. $\beta*(1-mult_t)$). It looks like we could separate into 3 different time periods corresponding to different $\beta$'s (initial exponential growth from days 0-30, declines in growth due to social distancing from days 30 to 65-70, and a recent uptick from days 65-70 on). We could thus start off with a multiplier equal to 0 (in the initial period; $\ mult_1=0$) and calibrate $\beta, mult_2, mult_3$. Note that we can incorporate time-varying parameters in the XYZ model code through the use of *if* statements or by referencing dataframes indexed by time. 

A more robust way to handle this would be to also calibrate how many changepoints there are and when the changepoints occur. This could also be incorporated into the same calibration procedure we are using now (by looping over possible time steps for the change points, for example) but calibration might start to get slow at this point and there are more efficient ways to handle this, for example through optimization algorithms (such as the *optim* function in R) or statistical analysis on the data pre-calibration to determine changepoints. Discussion of these methods goes a bit beyond the scope of what we've covered in class. 






